{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPsHhjV+vN37fHYpWCa/XPP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/illusoryTwin/InnoML/blob/main/task2/ML_task2_3004.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NVaD4bZtrfn",
        "outputId": "4292c2b6-86cf-4397-8e51-3be92ea96767"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 4s 0us/step\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import models, layers\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Load CIFAR-10 data\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "\n",
        "# Normalize pixel values of the images\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# One-hot encode the labels\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import models, layers\n",
        "\n",
        "import cv2\n",
        "# Define a function to apply blur to images\n",
        "\n",
        "# Define data augmentation parameters including the blur\n",
        "datagen = ImageDataGenerator(\n",
        "    shear_range=0.1,  # Shear transformation\n",
        "    rotation_range=15,  # Rotation\n",
        "    channel_shift_range=0.1,  # Color filtering\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# # Define data augmentation parameters\n",
        "# datagen = ImageDataGenerator(\n",
        "#     shear_range=0.1,  # Shear transformation\n",
        "#     rotation_range=15,  # Rotation\n",
        "#     # width_shift_range=0.1,  # Width shift\n",
        "#     # height_shift_range=0.1,  # Height shift\n",
        "#     # zoom_range=0.1,  # Zoom\n",
        "#     channel_shift_range=0.1,  # Color filtering\n",
        "#     # horizontal_flip=True,\n",
        "#     # fill_mode='nearest'\n",
        "# )\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(0.25),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(0.25),\n",
        "    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(0.25),\n",
        "    layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(0.25),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n"
      ],
      "metadata": {
        "id": "qQnhUwG0uGnk"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "\n",
        "class CosineAnnealingScheduler(Callback):\n",
        "    def __init__(self, lr_min, lr_max, T, T_mul=1):\n",
        "        super(CosineAnnealingScheduler, self).__init__()\n",
        "        self.lr_min = lr_min\n",
        "        self.lr_max = lr_max\n",
        "        self.T = T\n",
        "        self.T_mul = T_mul\n",
        "        self.cycle = 0\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        if epoch == 0:\n",
        "            self.cycle = 0\n",
        "        else:\n",
        "            self.cycle += 1\n",
        "            if self.cycle % self.T == 0:\n",
        "                self.lr_max *= 0.5\n",
        "                self.cycle = 0\n",
        "                self.T *= self.T_mul\n",
        "\n",
        "        lr = self.lr_min + 0.5 * (self.lr_max - self.lr_min) * (1 + np.cos(np.pi * self.cycle / self.T))\n",
        "        self.model.optimizer.lr = lr\n",
        "\n",
        "# Define initial learning rate and maximum learning rate\n",
        "lr_min = 0.001\n",
        "lr_max = 0.01\n",
        "\n",
        "# Define number of epochs for each cycle and multiplier for each cycle\n",
        "T = 10\n",
        "T_mul = 2\n",
        "\n",
        "# Create the CosineAnnealingScheduler\n",
        "cosine_annealing_scheduler = CosineAnnealingScheduler(lr_min, lr_max, T, T_mul)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Define callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train the model with data augmentation and the cosine annealing scheduler\n",
        "history = model.fit(datagen.flow(x_train, y_train, batch_size=256),\n",
        "                    epochs=100,\n",
        "                    validation_data=(x_test, y_test),\n",
        "                    callbacks=[early_stopping, cosine_annealing_scheduler])\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f'Test loss: {test_loss}, Test accuracy: {test_acc}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTSfn1N_uLsm",
        "outputId": "308763df-a6e5-4cc5-809c-7e00cd07c5aa"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "196/196 [==============================] - 39s 173ms/step - loss: 0.5937 - accuracy: 0.7955 - val_loss: 1.8728 - val_accuracy: 0.6101\n",
            "Epoch 2/100\n",
            "196/196 [==============================] - 33s 170ms/step - loss: 0.5493 - accuracy: 0.8102 - val_loss: 0.8298 - val_accuracy: 0.7610\n",
            "Epoch 3/100\n",
            "196/196 [==============================] - 34s 171ms/step - loss: 0.5174 - accuracy: 0.8205 - val_loss: 0.7220 - val_accuracy: 0.7712\n",
            "Epoch 4/100\n",
            "196/196 [==============================] - 35s 178ms/step - loss: 0.4633 - accuracy: 0.8394 - val_loss: 0.6023 - val_accuracy: 0.8147\n",
            "Epoch 5/100\n",
            "196/196 [==============================] - 33s 166ms/step - loss: 0.4282 - accuracy: 0.8511 - val_loss: 0.5308 - val_accuracy: 0.8336\n",
            "Epoch 6/100\n",
            "196/196 [==============================] - 34s 171ms/step - loss: 0.3763 - accuracy: 0.8689 - val_loss: 0.5224 - val_accuracy: 0.8332\n",
            "Epoch 7/100\n",
            "196/196 [==============================] - 34s 172ms/step - loss: 0.3321 - accuracy: 0.8845 - val_loss: 0.5087 - val_accuracy: 0.8418\n",
            "Epoch 8/100\n",
            "196/196 [==============================] - 33s 169ms/step - loss: 0.2884 - accuracy: 0.8987 - val_loss: 0.5080 - val_accuracy: 0.8440\n",
            "Epoch 9/100\n",
            "196/196 [==============================] - 34s 171ms/step - loss: 0.2514 - accuracy: 0.9115 - val_loss: 0.5039 - val_accuracy: 0.8485\n",
            "Epoch 10/100\n",
            "196/196 [==============================] - 34s 172ms/step - loss: 0.2396 - accuracy: 0.9135 - val_loss: 0.4997 - val_accuracy: 0.8523\n",
            "Epoch 11/100\n",
            "196/196 [==============================] - 32s 165ms/step - loss: 0.3013 - accuracy: 0.8947 - val_loss: 0.5415 - val_accuracy: 0.8410\n",
            "Epoch 12/100\n",
            "196/196 [==============================] - 33s 170ms/step - loss: 0.2994 - accuracy: 0.8948 - val_loss: 0.6048 - val_accuracy: 0.8184\n",
            "Epoch 13/100\n",
            "196/196 [==============================] - 34s 176ms/step - loss: 0.2888 - accuracy: 0.8986 - val_loss: 0.5458 - val_accuracy: 0.8412\n",
            "Epoch 14/100\n",
            "196/196 [==============================] - 33s 167ms/step - loss: 0.2783 - accuracy: 0.9022 - val_loss: 0.5923 - val_accuracy: 0.8219\n",
            "Epoch 15/100\n",
            "196/196 [==============================] - 34s 172ms/step - loss: 0.2663 - accuracy: 0.9072 - val_loss: 0.4950 - val_accuracy: 0.8495\n",
            "Epoch 16/100\n",
            "196/196 [==============================] - 33s 167ms/step - loss: 0.2502 - accuracy: 0.9108 - val_loss: 0.5284 - val_accuracy: 0.8413\n",
            "Epoch 17/100\n",
            "196/196 [==============================] - 33s 170ms/step - loss: 0.2387 - accuracy: 0.9154 - val_loss: 0.5676 - val_accuracy: 0.8400\n",
            "Epoch 18/100\n",
            "196/196 [==============================] - 33s 170ms/step - loss: 0.2233 - accuracy: 0.9206 - val_loss: 0.5594 - val_accuracy: 0.8435\n",
            "Epoch 19/100\n",
            "196/196 [==============================] - 33s 166ms/step - loss: 0.2188 - accuracy: 0.9229 - val_loss: 0.5479 - val_accuracy: 0.8440\n",
            "Epoch 20/100\n",
            "196/196 [==============================] - 34s 172ms/step - loss: 0.2016 - accuracy: 0.9291 - val_loss: 0.5403 - val_accuracy: 0.8471\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.4950 - accuracy: 0.8495\n",
            "Test loss: 0.4949704706668854, Test accuracy: 0.8495000004768372\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load test images and labels\n",
        "test_images = np.load(\"task_2_test_images.npy\")\n",
        "test_labels = np.load(\"task_2_test_labels.npy\")\n",
        "\n",
        "# Normalize pixel values\n",
        "test_images = test_images.astype('float32') / 255.0\n",
        "# Convert test labels to one-hot encoded format (if needed)\n",
        "test_labels = to_categorical(test_labels, num_classes=10)\n",
        "\n",
        "# Evaluate the model on the test dataset\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(f'Test loss: {test_loss}, Test accuracy: {test_acc}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cc6f7TWFuPsX",
        "outputId": "95ae3f5f-0906-4ab9-a10a-0e1b739a0401"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "312/312 [==============================] - 2s 5ms/step - loss: 1.0987 - accuracy: 0.7086\n",
            "Test loss: 1.098737120628357, Test accuracy: 0.7086179256439209\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Predict probabilities for the test set\n",
        "y_pred_probs = model.predict(test_images)\n",
        "\n",
        "# Calculate AUC-ROC score\n",
        "auc_roc = roc_auc_score(test_labels, y_pred_probs)\n",
        "\n",
        "print(f'AUC-ROC score: {auc_roc}')\n"
      ],
      "metadata": {
        "id": "EY4GCQYJHUu-",
        "outputId": "4356ce61-d5af-4f7a-aa5a-9fc5b7fe8972",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "312/312 [==============================] - 1s 3ms/step\n",
            "AUC-ROC score: 0.953846314498565\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer learning"
      ],
      "metadata": {
        "id": "P5X6rgY-Ehn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "import os\n",
        "# print(os.listdir(\"../input\"))\n",
        "\n",
        "# Any results you write to the current directory are saved as output.\n",
        "\n",
        "#Import standard libraries\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "''' to learn more about itertools visit\n",
        "    https://medium.com/@jasonrigden/a-guide-to-python-itertools-82e5a306cdf8'''\n",
        "import itertools\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "#Import keras functions\n",
        "\n",
        "from keras import Sequential\n",
        "\n",
        "'''Since we are using transfer learning let's import the model that we want to implement.Let's use VGG 19(19 layers) and Resnet-50 (50 layers of residual units).\n",
        "Residual units allow us to add more layers onto the model without a degradation in accuracy.\n",
        "Let's try and compare the accuracy of the 2 models and see if the addtional layers do make a significant difference. '''\n",
        "\n",
        "from keras.applications import VGG19,ResNet50\n",
        "\n",
        "'Import the datagenerator to augment images'\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "'''Import the optimizers and leanring rate annealer (which will reduce the learning rate once a particular metric we choose(in this case validation error)\n",
        "does not reduce after a user defined number of epochs)'''\n",
        "from keras.optimizers import SGD,Adam\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "'Lastly import the final layers that will be added on top of the base model'\n",
        "from keras.layers import Flatten,Dense,BatchNormalization,Activation,Dropout\n",
        "\n",
        "'Import to_categorical from the keras utils package to one hot encode the labels'\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "#Import dataset\n",
        "from keras.datasets import cifar10\n",
        "\n",
        "#Divide the data in Train, Validation and Test Datasets\n",
        "'I had to turn the Internet setting to on to download load the dataset'\n",
        "(x_train,y_train),(x_test,y_test)=cifar10.load_data()\n",
        "\n",
        "x_train,x_val,y_train,y_val=train_test_split(x_train,y_train,test_size=.3)"
      ],
      "metadata": {
        "id": "Dvtdmu_uyTgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print((x_train.shape,y_train.shape))\n",
        "print((x_val.shape,y_val.shape))\n",
        "print((x_test.shape,y_test.shape))"
      ],
      "metadata": {
        "id": "GWcC4mZTEjgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train=to_categorical(y_train)\n",
        "y_val=to_categorical(y_val)\n",
        "y_test=to_categorical(y_test)\n",
        "\n",
        "# Lets print the dimensions one more time to see if things changed the way we expected\n",
        "\n",
        "print((x_train.shape,y_train.shape))\n",
        "print((x_val.shape,y_val.shape))\n",
        "print((x_test.shape,y_test.shape))\n",
        "\n"
      ],
      "metadata": {
        "id": "mGDdWWrPEwal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator = ImageDataGenerator(\n",
        "                                    rotation_range=2,\n",
        "                                    horizontal_flip=True,\n",
        "                                    zoom_range=.1 )\n",
        "\n",
        "val_generator = ImageDataGenerator(\n",
        "                                    rotation_range=2,\n",
        "                                    horizontal_flip=True,\n",
        "                                    zoom_range=.1)\n",
        "\n",
        "test_generator = ImageDataGenerator(\n",
        "                                    rotation_range=2,\n",
        "                                    horizontal_flip= True,\n",
        "                                    zoom_range=.1)\n",
        "\n",
        "#Fit the augmentation method to the data\n",
        "\n",
        "train_generator.fit(x_train)\n",
        "val_generator.fit(x_val)\n",
        "test_generator.fit(x_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "zneL3XUuEzVv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}